# -*- coding: utf-8 -*-
"""LLM_FineTunning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dvAF_rf7DYkMyLKZF7-nYZv051maaL7K
"""

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig, TrainingArguments, Trainer
import torch
import time
import evaluate
import pandas as pd
import numpy as np
import random

# Commented out IPython magic to ensure Python compatibility.
# %pip install  evaluate==0.4.0

data_set = load_dataset("knkarthick/dialogsum")

original_model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base",torch_dtype=torch.bfloat16)
tokernizer = AutoTokenizer.from_pretrained("google/flan-t5-base")

def trainable_layer(model):
  trainable_params=0
  all_params=0
  for _,params in model.named_parameters():
    all_params+=params.numel()
    if params.requires_grad:
      trainable_params+=params.numel()

  return f"Trainable pramerters : {trainable_params} \nTotal parameters: {all_params}"

print(trainable_layer(original_model))

index= random.randint(0,200)

dialogue= data_set['test'][index]['dialogue']
summary= data_set['test'][index]['summary']

prompt = f"""
Summarize the following conversation.

{dialogue}

Summary:
"""

inputs = tokernizer(prompt,return_tensors='pt')
outputs=tokernizer.decode(
    original_model.generate(
        inputs["input_ids"],
        max_new_tokens=200
    )[0],skip_special_tokens=True
)

dash="-"*100

print(f'INPUT PROMPT:\n{prompt}')
print(dash)
print(f'BASELINE HUMAN SUMMARY:\n{summary}\n')
print(dash)
print(f'MODEL GENERATION - ZERO SHOT:\n{outputs}')



"""# **Full Model Tunning - catastropic loss**"""

def tokenize_function(example):
  start_prompt="Summarize the following conversation.\n\n"
  end_prompt='\n\nSummary:  '

  prompt = [start_prompt + dialogue + end_prompt for dialogue in example['dialogue']]
  example['input_ids'] = tokernizer(prompt, padding="max_length", truncation=True, return_tensors="pt").input_ids
  example['labels'] = tokernizer(example["summary"], padding="max_length", truncation=True, return_tensors="pt").input_ids

  return example

tokenized_dataset = data_set.map(tokenize_function,batched=True)
print(tokenized_dataset)
tokenized_dataset = tokenized_dataset.remove_columns(['id','topic','dialogue','summary'])

tokenized_dataset

tokenized_dataset = tokenized_dataset.filter(lambda example, index: index % 100 ==0, with_indices=True)

print(f"Training: {tokenized_dataset['train'].shape}")
print(f"Validation: {tokenized_dataset['validation'].shape}")



"""Fine-Tuning the Model"""

logs=f"./dialogue-summary-traiing-{str(int(time.time()))}"

training_args = TrainingArguments(
    output_dir = logs,
    learning_rate=1e-5,
    num_train_epochs=1,
    weight_decay=0.01,
    logging_steps=1,
    max_steps=1
)

trainer = Trainer(
    model=original_model,
    args=training_args,
    train_dataset=tokenized_dataset['train'],
    eval_dataset=tokenized_dataset['validation']
)

# It crashes as it requires more ram to train complete model
trainer.train()

"""# Using Lora"""

from peft import LoraConfig, get_peft_model, TaskType

lora_config = LoraConfig(
    r=32,
    lora_alpha=32,
    target_modules=["q","v"],
    lora_dropout=0.05,
    bias='none',
    task_type=TaskType.SEQ_2_SEQ_LM
)

output_dir= f'./peft_training_log-{str(int(time.time()))}'

peft_training_args = TrainingArguments(
    output_dir=output_dir,
    auto_find_batch_size=True,
    learning_rate=1e-5,
    num_train_epochs=1,
    logging_steps=1,
    max_steps=1
)

peft_trainer = Trainer(
    model=original_model,
    args=peft_training_args,
    train_dataset=tokenized_dataset['train'],
    eval_dataset=tokenized_dataset['validation']
)

peft_trainer.train()



"""# Evaluate Model"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install rouge_score
rouge = evaluate.load('rouge')

